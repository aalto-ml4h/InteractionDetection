{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_array(array):\n",
    "    temp = array.argsort()\n",
    "    ranks = np.empty_like(temp)\n",
    "    ranks[temp] = np.arange(len(array))\n",
    "    return ranks\n",
    "def matrix2vec(matrix):\n",
    "    d = len(matrix)\n",
    "    interactions = np.zeros([int(d * (d-1) / 2),1])\n",
    "    k = 0\n",
    "    for i in range(d):\n",
    "        for j in range(i+1, d):\n",
    "            interactions[k] = matrix[i][j]\n",
    "            k = k + 1  \n",
    "    return interactions\n",
    "def matric2dic(hessian, K):\n",
    "    IS = {}\n",
    "    for i in range(len(hessian[0])):\n",
    "        for j in range(i+1, len(hessian[0])):\n",
    "            tmp = 0\n",
    "            interation = 'Interaction: '\n",
    "            interation = interation + str(i + 1) + ' ' + str(j + 1) + ' '\n",
    "            IS[interation] = hessian[i][j]\n",
    "    Sorted_IS = [(k, IS[k]) for k in sorted(IS, key=IS.get, reverse=True)]\n",
    "    return IS, Sorted_IS\n",
    "def vec2dic(vector, num_dim):\n",
    "    IS = {}\n",
    "    tmp = 0\n",
    "    for i in range(num_dim):\n",
    "        for j in range(i+1, num_dim):\n",
    "            interation = 'Interaction: '\n",
    "            interation = interation + str(i + 1) + ' ' + str(j + 1) + ' '\n",
    "            IS[interation] = vector[tmp]\n",
    "            tmp = tmp + 1\n",
    "    Sorted_IS = [(k, IS[k]) for k in sorted(IS, key=IS.get, reverse=True)]\n",
    "    return IS, Sorted_IS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data (Energy efficiency Data Set from UCI)\n",
    "Data is from: A. Tsanas, A. Xifara: 'Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools', Energy and Buildings, Vol. 49, pp. 560-567, 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data preprocessing\n",
    "def data_generating_energy(traindf, testdf):\n",
    "\n",
    "    trainx = traindf.drop('Y2', axis=1).values; trainy = traindf['Y2'].values\n",
    "    testx = testdf.drop('Y2', axis=1).values; testy = testdf['Y2'].values\n",
    "\n",
    "    scalerx = preprocessing.MinMaxScaler().fit(trainx)\n",
    "    trainx = scalerx.transform(trainx)+ 0.5; testx = scalerx.transform(testx)+ 0.5\n",
    "\n",
    "    scalery = preprocessing.MinMaxScaler().fit(trainy.reshape(-1,1))\n",
    "    trainy = scalery.transform(trainy.reshape(-1,1)).reshape(1, -1); testy = scalery.transform(testy.reshape(-1,1)).reshape(1, -1)\n",
    "\n",
    "    # transfer data into tensor\n",
    "    x = torch.tensor(trainx, dtype = torch.float); y = torch.tensor(trainy[0], dtype = torch.float)\n",
    "    x_test = torch.tensor(testx, dtype = torch.float); y_test = torch.tensor(testy[0], dtype = torch.float)\n",
    "    \n",
    "    return x, y, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove none value\n",
    "df = pd.read_csv(\"energy.csv\", sep=\",\")\n",
    "df = df.drop(['Unnamed: 10','Unnamed: 11','Y1'], axis = 1)\n",
    "df = df.dropna(axis=0)\n",
    "# # split data into training and test set\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "traindf = df[msk]\n",
    "testdf = df[~msk]\n",
    "x, y, x_test, y_test = data_generating_energy(traindf, testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interaction effect measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Group Expected Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEH\n",
    "def inputHessian_dropout(mlp, r):\n",
    "    Hessian = []\n",
    "    output = mlp(r)\n",
    "    first = torch.autograd.grad(output, r, create_graph=True)\n",
    "    for i in range(len(r[0])):\n",
    "        gradient = torch.zeros(len(r[0]), dtype = torch.float)\n",
    "        gradient[i] = 1.0\n",
    "        second = torch.autograd.grad(first, r, grad_outputs=gradient.view(1,-1), retain_graph=True)\n",
    "        Hessian.append(second[0][0].tolist())\n",
    "    return Hessian\n",
    "def AEH(mlp, num_dim, data):\n",
    "    Hessian = np.zeros([num_dim, num_dim])\n",
    "    n_data = data.shape[0]\n",
    "    for i in range(n_data):\n",
    "        r = data[i].clone().detach().requires_grad_(True).view(1,-1)\n",
    "        Hessian = Hessian + np.array(inputHessian_dropout(mlp, r))\n",
    "    Hessian = abs(Hessian) / n_data \n",
    "    HS, sorted_HS = matric2dic(Hessian, 10)\n",
    "    return sorted_HS, Hessian\n",
    "\n",
    "## Group Expected Hessian\n",
    "def GEH(mlp, data, num_cluster):\n",
    "    kmeans = KMeans(n_clusters=num_cluster, random_state=0).fit(data)\n",
    "    data_sep = []\n",
    "    data_number = 0\n",
    "    for i in range(num_cluster):\n",
    "        data_i = data[(kmeans.labels_ == i).tolist()]; data_i = torch.tensor(data_i, dtype=torch.float)\n",
    "        data_sep.append(data_i)\n",
    "        data_number = data_number + data_i.shape[0]\n",
    "    mlp.eval()\n",
    "    Hessian = np.zeros([num_feature, num_feature])\n",
    "    for i in range(num_cluster):\n",
    "        _, hessian = AEH(mlp, num_feature, data = data_sep[i])\n",
    "        Hessian = Hessian + hessian * data_sep[i].shape[0] / data_number  \n",
    "    return Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bayesian Group Expected Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian GEH\n",
    "def MC_inputHessian(mlp, r, fixed_noise):\n",
    "    Hessian = []\n",
    "    output = mlp(r, fixed_noise)\n",
    "    first = torch.autograd.grad(output, r, create_graph=True)\n",
    "    for i in range(len(r[0])):\n",
    "        gradient = torch.zeros(len(r[0]), dtype = torch.float)\n",
    "        gradient[i] = 1.0\n",
    "        second = torch.autograd.grad(first, r, grad_outputs=gradient.view(1,-1), retain_graph=True)\n",
    "        Hessian.append(second[0][0].tolist())\n",
    "    return Hessian\n",
    "\n",
    "def MC_AEH(mlp, num_dim, fixed_noise, data):\n",
    "    Hessian = np.zeros([num_dim, num_dim])\n",
    "    n_data = data.shape[0]\n",
    "    for i in range(n_data):    \n",
    "        r = data[i].clone().detach().requires_grad_(True).view(1,-1)\n",
    "        Hessian = Hessian + np.array(MC_inputHessian(mlp, r, fixed_noise))\n",
    "    Hessian = abs(Hessian) / n_data\n",
    "    HS, sorted_HS = matric2dic(Hessian, 10)\n",
    "    return sorted_HS, Hessian\n",
    "\n",
    "def MC_GEH_cluster(mlp, data_sep, num_cluster):\n",
    "           \n",
    "    mlp.train()\n",
    "    fixed_noise = [torch.rand(8), torch.rand(20), torch.rand(10)]\n",
    "    Hessian = np.zeros([num_feature, num_feature])\n",
    "    data_number = 0\n",
    "    for i in range(num_cluster):\n",
    "        data_number = data_number + data_sep[i].shape[0]\n",
    "        \n",
    "    for i in range(num_cluster):\n",
    "        _, hessian = MC_AEH(mlp, num_feature, fixed_noise, data = data_sep[i])\n",
    "        Hessian = Hessian + hessian * data_sep[i].shape[0] / data_number\n",
    "        \n",
    "    return Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Determine the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_rank(Hessian0, Hessian1):\n",
    "    length = Hessian0.shape[0]\n",
    "    lti = np.tril_indices(length, -1)\n",
    "    attribution0 = Hessian0[lti] / np.sum(Hessian0[lti]); attribution1 = Hessian1[lti] / np.sum(Hessian1[lti])\n",
    "    rank0 = sort_array(attribution0); rank1 = sort_array(attribution1)\n",
    "    return np.sum(((attribution0-attribution1) ** 2 ) * ((rank0 - rank1) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_M(mlp, data, NUM_Cluster = 60):\n",
    "    increase = []\n",
    "    Hessian0 = GEH(mlp, data, 1)\n",
    "    for i in range(NUM_Cluster - 1):\n",
    "        num_cluster = i + 2\n",
    "        hessian = GEH(mlp, data, num_cluster)\n",
    "        increase.append(spearman_rank(Hessian0,hessian))\n",
    "        Hessian0 = hessian\n",
    "    return np.array(increase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Interaction via Concrete Dropout\n",
    "Gal, Yarin, Jiri Hron, and Alex Kendall. \"Concrete dropout.\" Advances in neural information processing systems. 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We add a main effect to improve the NN training, which could be omited as well.\n",
    "class Main_effect(nn.Module):\n",
    "    def __init__(self, num_dim):\n",
    "        super(Main_effect, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_dim, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different from the original concrete dropout, we control the random mask of each node manually by providing the noise. This is used in Bayesian GEH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concrete Dropout Layer\n",
    "class ConcreteDropout(nn.Module):\n",
    "    def __init__(self, size = 1, weight_regularizer=1e-6,\n",
    "                 dropout_regularizer=1e-5, init_min=0.1, init_max=0.1):\n",
    "        \n",
    "        super(ConcreteDropout, self).__init__()\n",
    "        \n",
    "        self.weight_regularizer = weight_regularizer\n",
    "        self.dropout_regularizer = dropout_regularizer\n",
    "        \n",
    "        init_min = np.log(init_min) - np.log(1. - init_min)\n",
    "        init_max = np.log(init_max) - np.log(1. - init_max)\n",
    "        \n",
    "        self.p_logit = nn.Parameter(torch.empty(size).uniform_(init_min, init_max))\n",
    "\n",
    "    def forward(self, x, layer, noise):\n",
    "        p = torch.sigmoid(self.p_logit)\n",
    "            \n",
    "        out = layer(self._concrete_dropout(x, p, noise))\n",
    "\n",
    "        sum_of_square = 0\n",
    "        \n",
    "        network_weights = torch.sum(torch.sum(torch.pow(layer.weight, 2), 0) / (1 - p))\n",
    "        network_bias = torch.sum(torch.pow(layer.bias, 2))   \n",
    "            \n",
    "        weights_regularizer = self.weight_regularizer * (network_bias + network_weights)\n",
    "\n",
    "        dropout_regularizer = p * torch.log(p) + (1. - p) * torch.log(1. - p)\n",
    "        dropout_regularizer = self.dropout_regularizer * torch.sum(dropout_regularizer)\n",
    "\n",
    "        regularization = weights_regularizer + dropout_regularizer\n",
    "\n",
    "        return out, regularization\n",
    "        \n",
    "    def _concrete_dropout(self, x, p, noise):\n",
    "        eps = 1e-7\n",
    "        temp = 0.1\n",
    "        \n",
    "        if type(noise) == int: ## shape of unif_noise is [num_data, dim]\n",
    "            unif_noise = torch.rand_like(x)\n",
    "        else:\n",
    "            unif_noise = noise\n",
    "            \n",
    "        drop_prob = (torch.log(p + eps)\n",
    "                    - torch.log(1 - p + eps)\n",
    "                    + torch.log(unif_noise + eps)\n",
    "                    - torch.log(1 - unif_noise + eps))\n",
    "        \n",
    "        drop_prob = torch.sigmoid(drop_prob / temp)\n",
    "        random_tensor = 1 - drop_prob\n",
    "        retain_prob = 1 - p\n",
    "        \n",
    "        x  = torch.mul(x, random_tensor)\n",
    "        x /= retain_prob\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP with concrete dropout layer\n",
    "class Model_NC_softplus(nn.Module):\n",
    "    def __init__(self, weight_regularizer, dropout_regularizer):\n",
    "        super(Model_NC_softplus, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 20)\n",
    "        self.linear2 = nn.Linear(20, 10)\n",
    "        self.linear3 = nn.Linear(10, 1)\n",
    "\n",
    "        self.conc_drop1 = ConcreteDropout(size = 8,  weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer)\n",
    "        self.conc_drop2 = ConcreteDropout(size = 20, weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer)\n",
    "        self.conc_drop3 = ConcreteDropout(size = 10, weight_regularizer=weight_regularizer,\n",
    "                                             dropout_regularizer=dropout_regularizer)        \n",
    "    def forward(self, x, NOISE = [0,0,0]): ## if the noise is not given, then generate a standard Gaussian noise\n",
    "        softplus = nn.Softplus(beta = 10)\n",
    "        if self.training:\n",
    "            regularization = torch.empty(3)\n",
    "\n",
    "            x1, regularization[0] = self.conc_drop1(x, self.linear1, NOISE[0])\n",
    "            x1 = softplus(x1)\n",
    "            x2, regularization[1] = self.conc_drop2(x1, self.linear2, NOISE[1])\n",
    "            x2 = softplus(x2)\n",
    "            output, regularization[2] = self.conc_drop3(x2, self.linear3, NOISE[2])\n",
    "            return output, regularization.sum()\n",
    "        else:\n",
    "            x = softplus(self.linear1(x))\n",
    "            x = softplus(self.linear2(x))\n",
    "            x = self.linear3(x)            \n",
    "            return x\n",
    "def heteroscedastic_loss(true, output):\n",
    "    return torch.mean(torch.sum((true - output)**2, 1), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data, num_feature = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters:\n",
    "learning_rate = 0.001;\n",
    "l = 1e-7\n",
    "wr = l**2. / num_data \n",
    "dr = 0.01 * 2 / num_data \n",
    "batch_size = 50; num_epoch = 2000;\n",
    "tolerance = 0.01; patience = 20;\n",
    "anneling = 10 * num_data / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Train a Concrete dropout NN\n",
    "About the trainining a Concrete dropout NN, please refer to [Gal et al., 2017] https://github.com/yaringal/ConcreteDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance, patience):\n",
    "        self.tolerance = tolerance\n",
    "        self.patience = patience\n",
    "    def stop_criterion(self, val_errors):\n",
    "        if len(val_errors) < self.patience + 1:\n",
    "            return False\n",
    "        else:\n",
    "            current_best = min(val_errors[:-self.patience])\n",
    "            current_stop = True\n",
    "            for i in range(self.patience):\n",
    "                current_stop = current_stop and (val_errors[-i-1] - current_best > self.tolerance)\n",
    "            return current_stop\n",
    "def training_CD_soft(mlp, main_effect, x, y, x_test, y_test, learning_rate = 0.001, anneling = 1000, batch_size = 50, num_epoch=1000, tolerance=0.002, patience = 20):\n",
    "    parameters = set(main_effect.parameters()) | set(mlp.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr = learning_rate)\n",
    "    early_stop = EarlyStopping(tolerance, patience)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "\n",
    "    num_data, num_dim = x.shape\n",
    "    y = y.view(-1, 1)\n",
    "    data = torch.cat((x, y), 1)\n",
    "    \n",
    "    annel_index = 0\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        # permuate the data\n",
    "        data_perm = data[torch.randperm(len(data))]\n",
    "        x = data_perm[:, 0:-1]\n",
    "        y = data_perm[:, -1]\n",
    "\n",
    "        for index in range(int(num_data/batch_size)):\n",
    "            # data comes in\n",
    "            inputs = x[index*batch_size : (index+1)*batch_size]\n",
    "            labels = y[index*batch_size : (index+1)*batch_size].view(-1,1)\n",
    "\n",
    "            # initialize the gradient of optimizer\n",
    "            optimizer.zero_grad()\n",
    "            mlp.train()\n",
    "            # calculate the loss function\n",
    "            output_mlp, reg = mlp(inputs)\n",
    "           # loss with var   \n",
    "#             output_mlp, var, reg = mlp(inputs)          \n",
    "#             loss = heteroscedastic_loss(labels, output_mlp + main_effect(inputs), var) + reg\n",
    "\n",
    "            # calculate the loss function\n",
    "            coef_annel = min(1, 0.01 + annel_index / anneling)\n",
    "        \n",
    "            loss = heteroscedastic_loss(labels, output_mlp + main_effect(inputs)) + coef_annel * reg\n",
    "            # backpropogate the gradient     \n",
    "            loss.backward()\n",
    "            # optimize with SGD\n",
    "            optimizer.step()\n",
    "            \n",
    "            annel_index += 1\n",
    "        # train and validation loss\n",
    "        mlp.eval()\n",
    "        train_errors.append(criterion(mlp.forward(x) + main_effect.forward(x), y.view(-1,1)))\n",
    "        val_errors.append(criterion(mlp.forward(x_test) + main_effect.forward(x_test), y_test.view(-1,1)))\n",
    "\n",
    "        # determine if early stop\n",
    "        if early_stop.stop_criterion(val_errors):\n",
    "            print(val_errors[epoch])\n",
    "            print('Stop after %d epochs' % epoch)\n",
    "            break\n",
    "\n",
    "        if (epoch % 100) == 0:\n",
    "            print('EPOCH %d: TRAIN LOSS: %.4f; VAL LOSS IS: %.5f.'% (epoch+1, train_errors[epoch], val_errors[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1: TRAIN LOSS: 0.2300; VAL LOSS IS: 0.22148.\n",
      "EPOCH 11: TRAIN LOSS: 0.0177; VAL LOSS IS: 0.01502.\n",
      "EPOCH 21: TRAIN LOSS: 0.0140; VAL LOSS IS: 0.01162.\n",
      "EPOCH 31: TRAIN LOSS: 0.0127; VAL LOSS IS: 0.01040.\n",
      "EPOCH 41: TRAIN LOSS: 0.0118; VAL LOSS IS: 0.00942.\n",
      "EPOCH 51: TRAIN LOSS: 0.0111; VAL LOSS IS: 0.00880.\n",
      "EPOCH 61: TRAIN LOSS: 0.0104; VAL LOSS IS: 0.00841.\n",
      "EPOCH 71: TRAIN LOSS: 0.0096; VAL LOSS IS: 0.00768.\n",
      "EPOCH 81: TRAIN LOSS: 0.0092; VAL LOSS IS: 0.00736.\n",
      "EPOCH 91: TRAIN LOSS: 0.0091; VAL LOSS IS: 0.00731.\n",
      "EPOCH 101: TRAIN LOSS: 0.0088; VAL LOSS IS: 0.00708.\n",
      "EPOCH 111: TRAIN LOSS: 0.0085; VAL LOSS IS: 0.00689.\n",
      "EPOCH 121: TRAIN LOSS: 0.0084; VAL LOSS IS: 0.00679.\n",
      "EPOCH 131: TRAIN LOSS: 0.0084; VAL LOSS IS: 0.00685.\n",
      "EPOCH 141: TRAIN LOSS: 0.0082; VAL LOSS IS: 0.00677.\n",
      "EPOCH 151: TRAIN LOSS: 0.0080; VAL LOSS IS: 0.00666.\n",
      "EPOCH 161: TRAIN LOSS: 0.0079; VAL LOSS IS: 0.00660.\n",
      "EPOCH 171: TRAIN LOSS: 0.0078; VAL LOSS IS: 0.00648.\n",
      "EPOCH 181: TRAIN LOSS: 0.0078; VAL LOSS IS: 0.00672.\n",
      "EPOCH 191: TRAIN LOSS: 0.0076; VAL LOSS IS: 0.00650.\n",
      "EPOCH 201: TRAIN LOSS: 0.0075; VAL LOSS IS: 0.00653.\n",
      "EPOCH 211: TRAIN LOSS: 0.0074; VAL LOSS IS: 0.00641.\n",
      "EPOCH 221: TRAIN LOSS: 0.0075; VAL LOSS IS: 0.00669.\n",
      "EPOCH 231: TRAIN LOSS: 0.0073; VAL LOSS IS: 0.00645.\n",
      "EPOCH 241: TRAIN LOSS: 0.0072; VAL LOSS IS: 0.00642.\n",
      "EPOCH 251: TRAIN LOSS: 0.0072; VAL LOSS IS: 0.00646.\n",
      "EPOCH 261: TRAIN LOSS: 0.0071; VAL LOSS IS: 0.00646.\n",
      "EPOCH 271: TRAIN LOSS: 0.0071; VAL LOSS IS: 0.00645.\n",
      "EPOCH 281: TRAIN LOSS: 0.0070; VAL LOSS IS: 0.00642.\n",
      "EPOCH 291: TRAIN LOSS: 0.0069; VAL LOSS IS: 0.00634.\n",
      "EPOCH 301: TRAIN LOSS: 0.0069; VAL LOSS IS: 0.00649.\n",
      "EPOCH 311: TRAIN LOSS: 0.0068; VAL LOSS IS: 0.00642.\n",
      "EPOCH 321: TRAIN LOSS: 0.0069; VAL LOSS IS: 0.00646.\n",
      "EPOCH 331: TRAIN LOSS: 0.0068; VAL LOSS IS: 0.00645.\n",
      "EPOCH 341: TRAIN LOSS: 0.0067; VAL LOSS IS: 0.00649.\n",
      "EPOCH 351: TRAIN LOSS: 0.0067; VAL LOSS IS: 0.00645.\n",
      "EPOCH 361: TRAIN LOSS: 0.0066; VAL LOSS IS: 0.00641.\n",
      "EPOCH 371: TRAIN LOSS: 0.0066; VAL LOSS IS: 0.00659.\n",
      "EPOCH 381: TRAIN LOSS: 0.0065; VAL LOSS IS: 0.00635.\n",
      "EPOCH 391: TRAIN LOSS: 0.0064; VAL LOSS IS: 0.00639.\n",
      "EPOCH 401: TRAIN LOSS: 0.0063; VAL LOSS IS: 0.00633.\n",
      "EPOCH 411: TRAIN LOSS: 0.0063; VAL LOSS IS: 0.00625.\n",
      "EPOCH 421: TRAIN LOSS: 0.0062; VAL LOSS IS: 0.00631.\n",
      "EPOCH 431: TRAIN LOSS: 0.0063; VAL LOSS IS: 0.00663.\n",
      "EPOCH 441: TRAIN LOSS: 0.0060; VAL LOSS IS: 0.00611.\n",
      "EPOCH 451: TRAIN LOSS: 0.0058; VAL LOSS IS: 0.00610.\n",
      "EPOCH 461: TRAIN LOSS: 0.0061; VAL LOSS IS: 0.00685.\n",
      "EPOCH 471: TRAIN LOSS: 0.0056; VAL LOSS IS: 0.00605.\n",
      "EPOCH 481: TRAIN LOSS: 0.0055; VAL LOSS IS: 0.00597.\n",
      "EPOCH 491: TRAIN LOSS: 0.0055; VAL LOSS IS: 0.00607.\n",
      "EPOCH 501: TRAIN LOSS: 0.0054; VAL LOSS IS: 0.00602.\n",
      "EPOCH 511: TRAIN LOSS: 0.0053; VAL LOSS IS: 0.00586.\n",
      "EPOCH 521: TRAIN LOSS: 0.0050; VAL LOSS IS: 0.00550.\n",
      "EPOCH 531: TRAIN LOSS: 0.0048; VAL LOSS IS: 0.00550.\n",
      "EPOCH 541: TRAIN LOSS: 0.0047; VAL LOSS IS: 0.00558.\n",
      "EPOCH 551: TRAIN LOSS: 0.0049; VAL LOSS IS: 0.00573.\n",
      "EPOCH 561: TRAIN LOSS: 0.0047; VAL LOSS IS: 0.00541.\n",
      "EPOCH 571: TRAIN LOSS: 0.0044; VAL LOSS IS: 0.00514.\n",
      "EPOCH 581: TRAIN LOSS: 0.0046; VAL LOSS IS: 0.00529.\n",
      "EPOCH 591: TRAIN LOSS: 0.0043; VAL LOSS IS: 0.00531.\n",
      "EPOCH 601: TRAIN LOSS: 0.0041; VAL LOSS IS: 0.00498.\n",
      "EPOCH 611: TRAIN LOSS: 0.0041; VAL LOSS IS: 0.00530.\n",
      "EPOCH 621: TRAIN LOSS: 0.0040; VAL LOSS IS: 0.00497.\n",
      "EPOCH 631: TRAIN LOSS: 0.0038; VAL LOSS IS: 0.00485.\n",
      "EPOCH 641: TRAIN LOSS: 0.0039; VAL LOSS IS: 0.00465.\n",
      "EPOCH 651: TRAIN LOSS: 0.0038; VAL LOSS IS: 0.00476.\n",
      "EPOCH 661: TRAIN LOSS: 0.0036; VAL LOSS IS: 0.00457.\n",
      "EPOCH 671: TRAIN LOSS: 0.0037; VAL LOSS IS: 0.00460.\n",
      "EPOCH 681: TRAIN LOSS: 0.0035; VAL LOSS IS: 0.00449.\n",
      "EPOCH 691: TRAIN LOSS: 0.0035; VAL LOSS IS: 0.00448.\n",
      "EPOCH 701: TRAIN LOSS: 0.0033; VAL LOSS IS: 0.00449.\n",
      "EPOCH 711: TRAIN LOSS: 0.0036; VAL LOSS IS: 0.00438.\n",
      "EPOCH 721: TRAIN LOSS: 0.0035; VAL LOSS IS: 0.00440.\n",
      "EPOCH 731: TRAIN LOSS: 0.0037; VAL LOSS IS: 0.00461.\n",
      "EPOCH 741: TRAIN LOSS: 0.0033; VAL LOSS IS: 0.00439.\n",
      "EPOCH 751: TRAIN LOSS: 0.0034; VAL LOSS IS: 0.00436.\n",
      "EPOCH 761: TRAIN LOSS: 0.0033; VAL LOSS IS: 0.00424.\n",
      "EPOCH 771: TRAIN LOSS: 0.0032; VAL LOSS IS: 0.00426.\n",
      "EPOCH 781: TRAIN LOSS: 0.0034; VAL LOSS IS: 0.00421.\n",
      "EPOCH 791: TRAIN LOSS: 0.0032; VAL LOSS IS: 0.00416.\n",
      "EPOCH 801: TRAIN LOSS: 0.0031; VAL LOSS IS: 0.00401.\n",
      "EPOCH 811: TRAIN LOSS: 0.0030; VAL LOSS IS: 0.00399.\n",
      "EPOCH 821: TRAIN LOSS: 0.0032; VAL LOSS IS: 0.00409.\n",
      "EPOCH 831: TRAIN LOSS: 0.0029; VAL LOSS IS: 0.00381.\n",
      "EPOCH 841: TRAIN LOSS: 0.0029; VAL LOSS IS: 0.00370.\n",
      "EPOCH 851: TRAIN LOSS: 0.0029; VAL LOSS IS: 0.00367.\n",
      "EPOCH 861: TRAIN LOSS: 0.0028; VAL LOSS IS: 0.00354.\n",
      "EPOCH 871: TRAIN LOSS: 0.0027; VAL LOSS IS: 0.00350.\n",
      "EPOCH 881: TRAIN LOSS: 0.0026; VAL LOSS IS: 0.00347.\n",
      "EPOCH 891: TRAIN LOSS: 0.0027; VAL LOSS IS: 0.00353.\n",
      "EPOCH 901: TRAIN LOSS: 0.0028; VAL LOSS IS: 0.00342.\n",
      "EPOCH 911: TRAIN LOSS: 0.0025; VAL LOSS IS: 0.00320.\n",
      "EPOCH 921: TRAIN LOSS: 0.0025; VAL LOSS IS: 0.00324.\n",
      "EPOCH 931: TRAIN LOSS: 0.0025; VAL LOSS IS: 0.00319.\n",
      "EPOCH 941: TRAIN LOSS: 0.0025; VAL LOSS IS: 0.00326.\n",
      "EPOCH 951: TRAIN LOSS: 0.0024; VAL LOSS IS: 0.00303.\n",
      "EPOCH 961: TRAIN LOSS: 0.0025; VAL LOSS IS: 0.00295.\n",
      "EPOCH 971: TRAIN LOSS: 0.0025; VAL LOSS IS: 0.00300.\n",
      "EPOCH 981: TRAIN LOSS: 0.0024; VAL LOSS IS: 0.00312.\n",
      "EPOCH 991: TRAIN LOSS: 0.0024; VAL LOSS IS: 0.00306.\n",
      "EPOCH 1001: TRAIN LOSS: 0.0024; VAL LOSS IS: 0.00303.\n",
      "EPOCH 1011: TRAIN LOSS: 0.0023; VAL LOSS IS: 0.00303.\n",
      "EPOCH 1021: TRAIN LOSS: 0.0023; VAL LOSS IS: 0.00289.\n",
      "EPOCH 1031: TRAIN LOSS: 0.0023; VAL LOSS IS: 0.00308.\n",
      "EPOCH 1041: TRAIN LOSS: 0.0022; VAL LOSS IS: 0.00285.\n",
      "EPOCH 1051: TRAIN LOSS: 0.0022; VAL LOSS IS: 0.00295.\n",
      "EPOCH 1061: TRAIN LOSS: 0.0022; VAL LOSS IS: 0.00287.\n",
      "EPOCH 1071: TRAIN LOSS: 0.0022; VAL LOSS IS: 0.00286.\n",
      "EPOCH 1081: TRAIN LOSS: 0.0022; VAL LOSS IS: 0.00259.\n",
      "EPOCH 1091: TRAIN LOSS: 0.0022; VAL LOSS IS: 0.00279.\n",
      "EPOCH 1101: TRAIN LOSS: 0.0022; VAL LOSS IS: 0.00271.\n",
      "EPOCH 1111: TRAIN LOSS: 0.0022; VAL LOSS IS: 0.00271.\n",
      "EPOCH 1121: TRAIN LOSS: 0.0022; VAL LOSS IS: 0.00273.\n",
      "EPOCH 1131: TRAIN LOSS: 0.0023; VAL LOSS IS: 0.00271.\n",
      "EPOCH 1141: TRAIN LOSS: 0.0022; VAL LOSS IS: 0.00275.\n",
      "EPOCH 1151: TRAIN LOSS: 0.0020; VAL LOSS IS: 0.00260.\n",
      "EPOCH 1161: TRAIN LOSS: 0.0021; VAL LOSS IS: 0.00273.\n",
      "EPOCH 1171: TRAIN LOSS: 0.0023; VAL LOSS IS: 0.00280.\n",
      "EPOCH 1181: TRAIN LOSS: 0.0021; VAL LOSS IS: 0.00261.\n",
      "EPOCH 1191: TRAIN LOSS: 0.0021; VAL LOSS IS: 0.00243.\n",
      "EPOCH 1201: TRAIN LOSS: 0.0021; VAL LOSS IS: 0.00264.\n",
      "EPOCH 1211: TRAIN LOSS: 0.0020; VAL LOSS IS: 0.00260.\n",
      "EPOCH 1221: TRAIN LOSS: 0.0020; VAL LOSS IS: 0.00251.\n",
      "EPOCH 1231: TRAIN LOSS: 0.0020; VAL LOSS IS: 0.00255.\n",
      "EPOCH 1241: TRAIN LOSS: 0.0021; VAL LOSS IS: 0.00267.\n",
      "EPOCH 1251: TRAIN LOSS: 0.0020; VAL LOSS IS: 0.00248.\n",
      "EPOCH 1261: TRAIN LOSS: 0.0021; VAL LOSS IS: 0.00254.\n",
      "EPOCH 1271: TRAIN LOSS: 0.0020; VAL LOSS IS: 0.00238.\n",
      "EPOCH 1281: TRAIN LOSS: 0.0020; VAL LOSS IS: 0.00247.\n",
      "EPOCH 1291: TRAIN LOSS: 0.0021; VAL LOSS IS: 0.00261.\n",
      "EPOCH 1301: TRAIN LOSS: 0.0021; VAL LOSS IS: 0.00245.\n",
      "EPOCH 1311: TRAIN LOSS: 0.0020; VAL LOSS IS: 0.00241.\n",
      "EPOCH 1321: TRAIN LOSS: 0.0019; VAL LOSS IS: 0.00263.\n",
      "EPOCH 1331: TRAIN LOSS: 0.0020; VAL LOSS IS: 0.00238.\n",
      "EPOCH 1341: TRAIN LOSS: 0.0019; VAL LOSS IS: 0.00250.\n",
      "EPOCH 1351: TRAIN LOSS: 0.0019; VAL LOSS IS: 0.00243.\n",
      "EPOCH 1361: TRAIN LOSS: 0.0019; VAL LOSS IS: 0.00244.\n",
      "EPOCH 1371: TRAIN LOSS: 0.0019; VAL LOSS IS: 0.00244.\n",
      "EPOCH 1381: TRAIN LOSS: 0.0018; VAL LOSS IS: 0.00244.\n",
      "EPOCH 1391: TRAIN LOSS: 0.0018; VAL LOSS IS: 0.00228.\n",
      "EPOCH 1401: TRAIN LOSS: 0.0019; VAL LOSS IS: 0.00241.\n",
      "EPOCH 1411: TRAIN LOSS: 0.0017; VAL LOSS IS: 0.00241.\n",
      "EPOCH 1421: TRAIN LOSS: 0.0018; VAL LOSS IS: 0.00223.\n",
      "EPOCH 1431: TRAIN LOSS: 0.0018; VAL LOSS IS: 0.00231.\n",
      "EPOCH 1441: TRAIN LOSS: 0.0018; VAL LOSS IS: 0.00226.\n",
      "EPOCH 1451: TRAIN LOSS: 0.0018; VAL LOSS IS: 0.00229.\n",
      "EPOCH 1461: TRAIN LOSS: 0.0017; VAL LOSS IS: 0.00216.\n",
      "EPOCH 1471: TRAIN LOSS: 0.0017; VAL LOSS IS: 0.00222.\n",
      "EPOCH 1481: TRAIN LOSS: 0.0016; VAL LOSS IS: 0.00218.\n",
      "EPOCH 1491: TRAIN LOSS: 0.0017; VAL LOSS IS: 0.00220.\n",
      "EPOCH 1501: TRAIN LOSS: 0.0016; VAL LOSS IS: 0.00217.\n",
      "EPOCH 1511: TRAIN LOSS: 0.0018; VAL LOSS IS: 0.00231.\n",
      "EPOCH 1521: TRAIN LOSS: 0.0016; VAL LOSS IS: 0.00227.\n",
      "EPOCH 1531: TRAIN LOSS: 0.0016; VAL LOSS IS: 0.00224.\n",
      "EPOCH 1541: TRAIN LOSS: 0.0017; VAL LOSS IS: 0.00220.\n",
      "EPOCH 1551: TRAIN LOSS: 0.0016; VAL LOSS IS: 0.00217.\n",
      "EPOCH 1561: TRAIN LOSS: 0.0016; VAL LOSS IS: 0.00216.\n",
      "EPOCH 1571: TRAIN LOSS: 0.0016; VAL LOSS IS: 0.00206.\n",
      "EPOCH 1581: TRAIN LOSS: 0.0016; VAL LOSS IS: 0.00210.\n",
      "EPOCH 1591: TRAIN LOSS: 0.0015; VAL LOSS IS: 0.00219.\n",
      "EPOCH 1601: TRAIN LOSS: 0.0015; VAL LOSS IS: 0.00215.\n",
      "EPOCH 1611: TRAIN LOSS: 0.0016; VAL LOSS IS: 0.00209.\n",
      "EPOCH 1621: TRAIN LOSS: 0.0015; VAL LOSS IS: 0.00208.\n",
      "EPOCH 1631: TRAIN LOSS: 0.0015; VAL LOSS IS: 0.00214.\n",
      "EPOCH 1641: TRAIN LOSS: 0.0016; VAL LOSS IS: 0.00208.\n",
      "EPOCH 1651: TRAIN LOSS: 0.0014; VAL LOSS IS: 0.00203.\n",
      "EPOCH 1661: TRAIN LOSS: 0.0015; VAL LOSS IS: 0.00204.\n",
      "EPOCH 1671: TRAIN LOSS: 0.0015; VAL LOSS IS: 0.00204.\n",
      "EPOCH 1681: TRAIN LOSS: 0.0015; VAL LOSS IS: 0.00196.\n",
      "EPOCH 1691: TRAIN LOSS: 0.0015; VAL LOSS IS: 0.00207.\n",
      "EPOCH 1701: TRAIN LOSS: 0.0014; VAL LOSS IS: 0.00184.\n",
      "EPOCH 1711: TRAIN LOSS: 0.0013; VAL LOSS IS: 0.00194.\n",
      "EPOCH 1721: TRAIN LOSS: 0.0014; VAL LOSS IS: 0.00194.\n",
      "EPOCH 1731: TRAIN LOSS: 0.0013; VAL LOSS IS: 0.00189.\n",
      "EPOCH 1741: TRAIN LOSS: 0.0014; VAL LOSS IS: 0.00201.\n",
      "EPOCH 1751: TRAIN LOSS: 0.0013; VAL LOSS IS: 0.00185.\n",
      "EPOCH 1761: TRAIN LOSS: 0.0014; VAL LOSS IS: 0.00183.\n",
      "EPOCH 1771: TRAIN LOSS: 0.0014; VAL LOSS IS: 0.00192.\n",
      "EPOCH 1781: TRAIN LOSS: 0.0013; VAL LOSS IS: 0.00186.\n",
      "EPOCH 1791: TRAIN LOSS: 0.0015; VAL LOSS IS: 0.00199.\n",
      "EPOCH 1801: TRAIN LOSS: 0.0012; VAL LOSS IS: 0.00180.\n",
      "EPOCH 1811: TRAIN LOSS: 0.0013; VAL LOSS IS: 0.00182.\n",
      "EPOCH 1821: TRAIN LOSS: 0.0014; VAL LOSS IS: 0.00194.\n",
      "EPOCH 1831: TRAIN LOSS: 0.0012; VAL LOSS IS: 0.00175.\n",
      "EPOCH 1841: TRAIN LOSS: 0.0012; VAL LOSS IS: 0.00183.\n",
      "EPOCH 1851: TRAIN LOSS: 0.0012; VAL LOSS IS: 0.00176.\n",
      "EPOCH 1861: TRAIN LOSS: 0.0013; VAL LOSS IS: 0.00193.\n",
      "EPOCH 1871: TRAIN LOSS: 0.0012; VAL LOSS IS: 0.00183.\n",
      "EPOCH 1881: TRAIN LOSS: 0.0012; VAL LOSS IS: 0.00180.\n",
      "EPOCH 1891: TRAIN LOSS: 0.0012; VAL LOSS IS: 0.00170.\n",
      "EPOCH 1901: TRAIN LOSS: 0.0012; VAL LOSS IS: 0.00175.\n",
      "EPOCH 1911: TRAIN LOSS: 0.0013; VAL LOSS IS: 0.00178.\n",
      "EPOCH 1921: TRAIN LOSS: 0.0012; VAL LOSS IS: 0.00166.\n",
      "EPOCH 1931: TRAIN LOSS: 0.0011; VAL LOSS IS: 0.00165.\n",
      "EPOCH 1941: TRAIN LOSS: 0.0012; VAL LOSS IS: 0.00171.\n",
      "EPOCH 1951: TRAIN LOSS: 0.0012; VAL LOSS IS: 0.00172.\n",
      "EPOCH 1961: TRAIN LOSS: 0.0013; VAL LOSS IS: 0.00170.\n",
      "EPOCH 1971: TRAIN LOSS: 0.0011; VAL LOSS IS: 0.00164.\n",
      "EPOCH 1981: TRAIN LOSS: 0.0011; VAL LOSS IS: 0.00168.\n",
      "EPOCH 1991: TRAIN LOSS: 0.0011; VAL LOSS IS: 0.00157.\n"
     ]
    }
   ],
   "source": [
    "mlp = Model_NC_softplus(wr, dr)\n",
    "main_effect = Main_effect(num_feature)\n",
    "training_CD_soft(mlp, main_effect, x, y, x_test, y_test, learning_rate, anneling, batch_size, num_epoch, tolerance, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load a pretrained Concrete dropout NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = Model_NC_softplus(wr, dr)\n",
    "main_effect = Main_effect(num_feature)\n",
    "mlp.load_state_dict(torch.load('mlp_CD_softReLU_Energy.pth', map_location = lambda storage, loc: storage))\n",
    "main_effect.load_state_dict(torch.load('main_effect_CD_softReLU_Energy.pth', map_location = lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Determine the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta_m = delta_M(mlp, x_test.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (4, 4))\n",
    "ax.plot(np.linspace(2, len(delta_m)+1, len(delta_m)), delta_m, '-', c = 'red')\n",
    "ax.axvline(x=15)\n",
    "ax.legend((r'$\\Delta_{M}^{2}$', 'M=15'), prop = {'size' : 14})\n",
    "\n",
    "ax.set(xlabel=r'$M$', ylabel=r'$\\Delta_{M}^{2}$',title=r'$\\Delta_{M}^{2}$' + ' in simulation data')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Estimate interaction effect as well as its uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imuncertainty(mlp, data, num_cluster, n_iter = 200):\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_cluster, random_state=0).fit(data)\n",
    "    data_sep = []\n",
    "    for i in range(num_cluster):\n",
    "        data_i = data[(kmeans.labels_ == i).tolist()]; data_i = torch.tensor(data_i, dtype=torch.float)\n",
    "        data_sep.append(data_i) \n",
    "\n",
    "    print('############# The 1st interation for uncertainty calculation')\n",
    "    # Initialize the first interaction\n",
    "    Hessian = MC_GEH_cluster(mlp, data_sep, num_cluster)\n",
    "    int_unc = matrix2vec(Hessian)\n",
    "\n",
    "    # Rest of Interaction\n",
    "    for num_int in range(n_iter - 1):\n",
    "        if ((num_int + 2) % 10 == 0):\n",
    "            print('############# The %dth interation for uncertainty calculation' %(num_int + 2))\n",
    "        Hessian = MC_GEH_cluster(mlp, data_sep, num_cluster)\n",
    "        int_unc_new = matrix2vec(Hessian)\n",
    "    \n",
    "        int_unc = np.concatenate((int_unc, int_unc_new), axis=1)\n",
    "\n",
    "    mean_interaction = np.mean(int_unc, axis = 1)\n",
    "    std_interaction = np.std(int_unc, axis = 1)\n",
    "    print('mean of each interaction is')\n",
    "    print(mean_interaction)\n",
    "    print('std of each interaction is')\n",
    "    print(std_interaction)\n",
    "    \n",
    "    return mean_interaction, std_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# The 1st interation for uncertainty calculation\n",
      "############# The 10th interation for uncertainty calculation\n",
      "############# The 20th interation for uncertainty calculation\n",
      "############# The 30th interation for uncertainty calculation\n",
      "############# The 40th interation for uncertainty calculation\n",
      "############# The 50th interation for uncertainty calculation\n",
      "############# The 60th interation for uncertainty calculation\n",
      "############# The 70th interation for uncertainty calculation\n",
      "############# The 80th interation for uncertainty calculation\n",
      "############# The 90th interation for uncertainty calculation\n",
      "############# The 100th interation for uncertainty calculation\n",
      "############# The 110th interation for uncertainty calculation\n",
      "############# The 120th interation for uncertainty calculation\n",
      "############# The 130th interation for uncertainty calculation\n",
      "############# The 140th interation for uncertainty calculation\n",
      "############# The 150th interation for uncertainty calculation\n",
      "############# The 160th interation for uncertainty calculation\n",
      "############# The 170th interation for uncertainty calculation\n",
      "############# The 180th interation for uncertainty calculation\n",
      "############# The 190th interation for uncertainty calculation\n",
      "############# The 200th interation for uncertainty calculation\n",
      "mean of each interaction is\n",
      "[0.15344965 0.72796945 0.29055817 0.22547909 0.09100937 0.0443669\n",
      " 0.07545745 0.32850892 0.13764551 0.10809638 0.05908322 0.02965455\n",
      " 0.0625729  1.12761857 0.77385346 0.13411448 0.05141141 0.13808444\n",
      " 0.314224   0.09669903 0.03662554 0.09471877 0.04181584 0.02924906\n",
      " 0.05650796 0.01636239 0.07654365 0.03895689]\n",
      "std of each interaction is\n",
      "[0.03268783 0.18752102 0.07374413 0.05504252 0.02165109 0.01049153\n",
      " 0.01804909 0.07525291 0.03200349 0.02538166 0.01408367 0.00622073\n",
      " 0.01575697 0.2986789  0.20578721 0.03060824 0.01101407 0.03621812\n",
      " 0.08694892 0.02509655 0.00896183 0.02401391 0.01427517 0.00686872\n",
      " 0.01329338 0.00394887 0.01930301 0.00834193]\n"
     ]
    }
   ],
   "source": [
    "num_cluster = 15\n",
    "mean_interaction, std_interaction = imuncertainty(mlp, x_test.numpy(), num_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Interaction: 3 4 ', 1.1276185688058544), ('Interaction: 3 5 ', 0.7738534631569487), ('Interaction: 1 3 ', 0.7279694476150499), ('Interaction: 2 3 ', 0.3285089157595713), ('Interaction: 4 5 ', 0.3142240000624243), ('Interaction: 1 4 ', 0.29055816967552917), ('Interaction: 1 5 ', 0.22547909396032442), ('Interaction: 1 2 ', 0.15344964767064517), ('Interaction: 3 8 ', 0.13808443643023552), ('Interaction: 2 4 ', 0.13764551229916144), ('Interaction: 3 6 ', 0.13411447704662968), ('Interaction: 2 5 ', 0.10809637768809566), ('Interaction: 4 6 ', 0.09669903455395094), ('Interaction: 4 8 ', 0.09471876884376122), ('Interaction: 1 6 ', 0.09100937483287436), ('Interaction: 6 8 ', 0.07654364620662787), ('Interaction: 1 8 ', 0.07545745086640929), ('Interaction: 2 8 ', 0.06257289598163941), ('Interaction: 2 6 ', 0.05908322224017471), ('Interaction: 5 8 ', 0.05650796306277628), ('Interaction: 3 7 ', 0.05141140880252742), ('Interaction: 1 7 ', 0.0443668955066896), ('Interaction: 5 6 ', 0.04181584432130275), ('Interaction: 7 8 ', 0.038956885058492846), ('Interaction: 4 7 ', 0.036625539891832704), ('Interaction: 2 7 ', 0.029654549457624246), ('Interaction: 5 7 ', 0.029249057684136368), ('Interaction: 6 7 ', 0.016362394141411604)]\n"
     ]
    }
   ],
   "source": [
    "IS, Sorted_IS = vec2dic(mean_interaction, num_feature)\n",
    "print(Sorted_IS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3/anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
